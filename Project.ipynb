{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c433ee9",
   "metadata": {},
   "source": [
    "## Project Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2aa6e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3a9babfe-3c9f-4cd7-94a2-c024c18030bb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 720ms :: artifacts dl 16ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3a9babfe-3c9f-4cd7-94a2-c024c18030bb\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/21ms)\n",
      "24/12/03 18:37:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, split, col, window\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(\"spark.jars.packages\", 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0') \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea83f5c3",
   "metadata": {},
   "source": [
    "Be sure to start the stream on Kafka!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d1102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType, TimestampType, DateType\n",
    "\n",
    "schema = StructType(\n",
    "      [\n",
    "        StructField(\"name\", StringType(), False),\n",
    "        StructField(\"price\", DoubleType(), False),\n",
    "        StructField(\"timestamp\", TimestampType(), False),\n",
    "      ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cdb079",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_server = \"kafka1:9092\"   \n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "lines = (spark.readStream                        # Get the DataStreamReader\n",
    "  .format(\"kafka\")                                 # Specify the source format as \"kafka\"\n",
    "  .option(\"kafka.bootstrap.servers\", kafka_server) # Configure the Kafka server name and port\n",
    "  .option(\"subscribe\", \"stock\")                       # Subscribe to the \"en\" Kafka topic \n",
    "  .option(\"startingOffsets\", \"earliest\")           # The start point when a query is started\n",
    "  .option(\"maxOffsetsPerTrigger\", 100)             # Rate limit on max offsets per trigger interval\n",
    "  .load()\n",
    "  .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"parsed_value\"))\n",
    "# Load the DataFrame\n",
    ")\n",
    "df = lines.select(\"parsed_value.*\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12ec0b9",
   "metadata": {},
   "source": [
    "## The assignment starts here\n",
    "\n",
    "You can create a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1f8b3e",
   "metadata": {},
   "source": [
    "## Select the N most valuable stocks in a window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e21afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql.functions import window, col, desc, rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define the window duration\n",
    "window_duration = \"80 seconds\"  # Set the window duration to 80 seconds\n",
    "\n",
    "# Define the processing function\n",
    "def process_batch_function(dataframe, epoch_id):\n",
    "    # Group the data by window and name, and calculate the max price for each stock\n",
    "    windowed_stocks = dataframe.groupBy(\n",
    "        window(col(\"timestamp\"), window_duration),\n",
    "        col(\"name\")\n",
    "    ).agg({\"price\": \"max\"}).withColumnRenamed(\"max(price)\", \"max_price\")\n",
    "\n",
    "    # Define the window specification for ranking\n",
    "    window_spec = Window.partitionBy(\"window\").orderBy(desc(\"max_price\"))\n",
    "\n",
    "    # Apply the rank function over the window specification\n",
    "    windowed_stocks = windowed_stocks.withColumn(\"rank\", rank().over(window_spec))\n",
    "\n",
    "    # Filter for the top N stocks\n",
    "    N = 10  # For example, find the top 10 stocks\n",
    "    top_n_stocks = windowed_stocks.where(col(\"rank\") <= N)\n",
    "\n",
    "    # Show the results for this micro-batch\n",
    "    print(f\"Batch ID: {epoch_id}\")\n",
    "    top_n_stocks.show(truncate=False)  # Avoid truncating the results for better readability\n",
    "\n",
    "# Apply the function to each micro-batch\n",
    "query = df.writeStream.foreachBatch(process_batch_function).start()\n",
    "\n",
    "# Wait for the streaming query to terminate\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307ee96e",
   "metadata": {},
   "source": [
    "## Select the stocks that lost value between two windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed0b397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, window, lag\n",
    "\n",
    "# Define the window duration\n",
    "window_duration = \"80 seconds\"  # 80 seconds window\n",
    "\n",
    "# Define the processing function\n",
    "def process_batch_function(dataframe, epoch_id):\n",
    "    # Group the data by window and name, and calculate the max price for each stock\n",
    "    windowed_stocks = dataframe.groupBy(\n",
    "        window(col(\"timestamp\"), window_duration),\n",
    "        col(\"name\")\n",
    "    ).agg({\"price\": \"max\"}).withColumnRenamed(\"max(price)\", \"max_price\")\n",
    "\n",
    "    # Extract window start for ordering\n",
    "    windowed_stocks = windowed_stocks.withColumn(\"window_start\", col(\"window.start\"))\n",
    "\n",
    "    # Define the window specification for calculating lag\n",
    "    window_spec = Window.partitionBy(\"name\").orderBy(\"window_start\")\n",
    "\n",
    "    # Add a lag column to calculate the previous max price\n",
    "    windowed_stocks = windowed_stocks.withColumn(\"prev_max_price\", lag(\"max_price\").over(window_spec))\n",
    "\n",
    "    # Filter for stocks that lost value\n",
    "    lost_value_stocks = windowed_stocks.where(col(\"max_price\") < col(\"prev_max_price\"))\n",
    "\n",
    "    # Show or store the results\n",
    "    print(f\"Batch ID: {epoch_id}\")\n",
    "    lost_value_stocks.show(truncate=False)  # Show full data without truncation\n",
    "\n",
    "# Apply the function to each micro-batch\n",
    "query = df.writeStream.foreachBatch(process_batch_function).start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa749f1",
   "metadata": {},
   "source": [
    "## Select the stock that gained the most (between windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee52f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, window, lead\n",
    "\n",
    "# Define the window duration\n",
    "window_duration = \"80 seconds\"  # 80 seconds window\n",
    "\n",
    "# Define the processing function\n",
    "def process_batch_function(dataframe, epoch_id):\n",
    "    # Group the data by window and name, and calculate the max price for each stock\n",
    "    windowed_stocks = dataframe.groupBy(\n",
    "        window(col(\"timestamp\"), window_duration),\n",
    "        col(\"name\")\n",
    "    ).agg({\"price\": \"max\"}).withColumnRenamed(\"max(price)\", \"max_price\")\n",
    "\n",
    "    # Extract window start for ordering\n",
    "    windowed_stocks = windowed_stocks.withColumn(\"window_start\", col(\"window.start\"))\n",
    "\n",
    "    # Define the window specification for calculating lead\n",
    "    window_spec = Window.partitionBy(\"name\").orderBy(\"window_start\")\n",
    "\n",
    "    # Add a lead column to calculate the next max price\n",
    "    windowed_stocks = windowed_stocks.withColumn(\"next_max_price\", lead(\"max_price\").over(window_spec))\n",
    "\n",
    "    # Filter for stocks that gained value\n",
    "    gained_value_stocks = windowed_stocks.where(col(\"max_price\") < col(\"next_max_price\"))\n",
    "\n",
    "    # Show or store the results\n",
    "    print(f\"Batch ID: {epoch_id}\")\n",
    "    gained_value_stocks.show(truncate=False)  # Show full data without truncation\n",
    "\n",
    "# Apply the function to each micro-batch\n",
    "query = df.writeStream.foreachBatch(process_batch_function).start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622ed685",
   "metadata": {},
   "source": [
    "## Implement a control that checks if a stock does not lose too much value in a period of time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442d6252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, col, when, window\n",
    "\n",
    "# Define the window duration\n",
    "window_duration = \"80 seconds\"  # 80 seconds window\n",
    "\n",
    "# Define the window specification for calculating lag\n",
    "window_spec = Window.partitionBy(\"name\").orderBy(\"window.start\")\n",
    "\n",
    "# Define the threshold for allowable loss (e.g., 5%)\n",
    "loss_threshold = 0.05\n",
    "\n",
    "# Define the processing function\n",
    "def process_batch_function(dataframe, epoch_id):\n",
    "    # Group the data by window and name, and calculate the max price for each stock\n",
    "    windowed_stocks = dataframe.groupBy(\n",
    "        window(col(\"timestamp\"), window_duration),\n",
    "        col(\"name\")\n",
    "    ).max(\"price\").withColumnRenamed(\"max(price)\", \"max_price\")\n",
    "\n",
    "    # Extract window start for ordering\n",
    "    windowed_stocks = windowed_stocks.withColumn(\"window_start\", col(\"window.start\"))\n",
    "\n",
    "    # Add a lag column to calculate the previous max price\n",
    "    windowed_stocks = windowed_stocks.withColumn(\n",
    "        \"prev_max_price\",\n",
    "        lag(\"max_price\").over(window_spec)\n",
    "    )\n",
    "\n",
    "    # Calculate the loss percentage for each stock within the window\n",
    "    windowed_stocks = windowed_stocks.withColumn(\n",
    "        \"loss_percentage\",\n",
    "        when(\n",
    "            col(\"prev_max_price\").isNotNull(),\n",
    "            (col(\"max_price\") - col(\"prev_max_price\")) / col(\"prev_max_price\")\n",
    "        ).otherwise(0.0)\n",
    "    )\n",
    "\n",
    "    # Filter for stocks that exceed the loss threshold\n",
    "    high_loss_stocks = windowed_stocks.where(col(\"loss_percentage\") < -loss_threshold)\n",
    "\n",
    "    # Show or store the results\n",
    "    print(f\"Batch ID: {epoch_id}\")\n",
    "    high_loss_stocks.show(truncate=False)\n",
    "\n",
    "# Apply the function to each micro-batch\n",
    "query = df.writeStream.foreachBatch(process_batch_function).start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfddea4",
   "metadata": {},
   "source": [
    "## Compute your assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f1f0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, window\n",
    "\n",
    "# Define the window duration\n",
    "window_duration = \"80 seconds\"  # 80 seconds window\n",
    "\n",
    "# Define the processing function\n",
    "def process_batch_function(dataframe, epoch_id):\n",
    "    # Group the data by window and calculate the total assets\n",
    "    total_assets_df = dataframe.groupBy(\n",
    "        window(col(\"timestamp\"), window_duration)\n",
    "    ).sum(\"price\").withColumnRenamed(\"sum(price)\", \"total_assets\")\n",
    "\n",
    "    # Show the results with batch identifier\n",
    "    print(f\"Batch ID: {epoch_id}\")\n",
    "    total_assets_df.show(truncate=False)\n",
    "\n",
    "# Apply the function to each micro-batch\n",
    "query = df.writeStream.foreachBatch(process_batch_function).start()\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f91e4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
